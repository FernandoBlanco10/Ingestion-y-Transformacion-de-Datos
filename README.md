# ğŸ“Š Data Ingestion & Analytics Pipeline (Data Engineering Project)

**Autor:** Fernando Blanco

Una colecciÃ³n completa de APIs REST para gestionar colecciones de canciones, implementadas en diferentes tecnologÃ­as y niveles de complejidad.

[![Node.js](https://img.shields.io/badge/Node.js-18+-green.svg)](https://nodejs.org)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-3.0-orange.svg)](https://flask.palletsprojects.com)
[![Express.js](https://img.shields.io/badge/Express.js-4.x-black.svg)](https://expressjs.com)

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un **pipeline bÃ¡sico de ingenierÃ­a de datos** que permite:

- Ingestar datos mediante una **API REST en Node.js**
- Limpiar y normalizar los datos usando **Python**
- Persistir los datos estructurados en **PostgreSQL**
- Analizar la informaciÃ³n mediante **Jupyter Notebooks**

El enfoque principal es simular un **flujo real de datos** desde la ingestiÃ³n hasta el anÃ¡lisis, aplicando buenas prÃ¡cticas comunes en proyectos de Data Engineering.

---

## ğŸ§± Arquitectura del proyecto
```bash
data-ingestion-project/
â”‚
â”œâ”€â”€ api-node/ # API REST para ingestiÃ³n de datos
â”‚ â”œâ”€â”€ routes/
â”‚ â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ utils/
â”‚ â””â”€â”€ app.js
â”‚
â”œâ”€â”€ service/ # Limpieza y normalizaciÃ³n de datos
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ database/
â”‚ â””â”€â”€ schema.sql # Script SQL para crear la base de datos
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ ventas_analysis.ipynb # AnÃ¡lisis de datos con Jupyter
â”‚
â””â”€â”€ README.md
```
---

## ğŸ”„ Flujo de datos

1. **Ingesta**
   - Los datos se envÃ­an mediante un endpoint REST (`/ingest`) usando Postman.
   - La API estÃ¡ construida con **Node.js + Express**.

2. **ValidaciÃ³n**
   - Se valida la estructura bÃ¡sica del payload antes de procesarlo.
   - Se evita insertar datos incompletos o mal formados.

3. **Limpieza y normalizaciÃ³n**
   - Los datos crudos son enviados a un servicio en **Python**.
   - Se aplican reglas de limpieza:
     - EliminaciÃ³n de valores nulos
     - NormalizaciÃ³n de texto
     - ConversiÃ³n de tipos
   - Este paso simula un **data processing layer** independiente.

4. **Persistencia**
   - Los datos limpios se almacenan en **PostgreSQL**.
   - El modelo estÃ¡ **normalizado** (clientes, productos, ventas).
   - Se usan claves primarias y forÃ¡neas.

5. **AnÃ¡lisis**
   - Se consulta PostgreSQL desde **Jupyter Notebook** usando `pandas` y `SQLAlchemy`.
   - Se realizan JOINs y anÃ¡lisis exploratorio (EDA).
   - No se modifica la base de datos desde el notebook.

---

## ğŸ—„ï¸ Base de datos

La base de datos se crea a partir de un archivo SQL versionado en el repositorio:

```bash
psql -U postgres -f database/schema.sql
```
**Decisiones de diseÃ±o**

- Uso de SERIAL para claves primarias.
- SeparaciÃ³n de entidades para evitar duplicidad.
- La base actÃºa como fuente de verdad (source of truth).

ğŸ““ **AnÃ¡lisis de datos**

- El anÃ¡lisis se realiza en Jupyter:
- ConexiÃ³n directa a PostgreSQL
 - Lectura de datos con SQL
- Transformaciones ligeras con pandas
- VisualizaciÃ³n bÃ¡sica
- Esto simula un entorno donde:
- La base es OLTP
- Jupyter se usa solo para anÃ¡lisis (OLAP)

ğŸ§  **Decisiones tÃ©cnicas relevantes**

SeparaciÃ³n de responsabilidades

- API â†’ ingestiÃ³n

- Python â†’ limpieza

- PostgreSQL â†’ persistencia

- Jupyter â†’ anÃ¡lisis
---
- No se limpian datos en la API
- La limpieza se delega a un servicio especializado.
- Esto facilita escalabilidad y mantenimiento.
- NormalizaciÃ³n de datos.
- Permite anÃ¡lisis correctos y JOINs eficientes.
- Uso de scripts SQL versionados
- Reproducibilidad del entorno.

## â–¶ï¸ CÃ³mo ejecutar y probar el proyecto

Este proyecto estÃ¡ diseÃ±ado para ejecutarse **localmente** y consta de **tres componentes principales**:

1. Base de datos en PostgreSQL
2. API de ingestiÃ³n en Node.js
3. API de limpieza de datos en Python

Cada componente debe levantarse manualmente.

---

## ğŸ§© Requisitos previos

AsegÃºrate de tener instalado:

- Git
- Node.js (v18 o superior recomendado)
- Python (3.9 o superior)
- PostgreSQL
- Postman (para pruebas)

---

 ğŸ“¥ **1. Clonar el repositorio**

```bash
git clone https://github.com/FernandoBlanco10/Ingestion-y-Transformacion-de-Datos.git
cd data-ingestion-project
```
ğŸ—„ï¸ **2. Crear la base de datos**

Desde la terminal de PostgreSQL (psql):

```bash
psql -U postgres -f database/schema.sql
```
Esto crearÃ¡:

- La base de datos
- Las tablas normalizadas necesarias para el proyecto

VerificaciÃ³n
```sql
\c data_ingestion_db
\dt
```

**ğŸ§¼ 3. Levantar la API de limpieza (Python)**

Este servicio se encarga de limpiar y normalizar los datos antes de insertarlos en la base de datos.

3.1 Instalar dependencias
Desde la carpeta service-python/:

```bash
cd service-python
pip install -r requirements.txt
```

3.2 Levantar la API de Python
```bash
python app.py
```
La API quedarÃ¡ escuchando en un puerto local (por ejemplo):

```arduino
http://localhost:5002
```

âš ï¸ Este servicio debe permanecer activo mientras se use la API de ingestiÃ³n.

VerificaciÃ³n
En el navegador o Postman:

```bash
GET http://localhost:5000/health
```
Respuesta esperada:

```json
{ "status": "ok" }
```

**ğŸš€ 4. Levantar la API de ingestiÃ³n (Node.js)**
Esta API recibe los datos crudos y los envÃ­a a la API de Python para su limpieza.

4.1 Instalar dependencias
Desde api-node/:

```bash
cd api-node
npm install
```
4.2 Levantar la API
```bash
npm start
```
La API quedarÃ¡ escuchando en:

```arduino
http://localhost:3000
```
**ğŸ“¤ 5. Probar la ingesta de datos (Postman)**

Endpoint
```bash
POST http://localhost:3000/ingest
```
Body (JSON)
```json
{
  "cliente": "Juan Perez",
  "email": "juan@mail.com",
  "producto": "Laptop",
  "categoria": "Tecnologia",
  "cantidad": 2,
  "precio_unitario": 15000
}
```
Flujo interno

* Node.js recibe el payload
* Node envÃ­a los datos a la API de Python
* Python limpia y normaliza
* Node inserta los datos en PostgreSQL

ğŸ§ª 6. Verificar datos en PostgreSQL
Desde psql:

```sql
SELECT * FROM clientes;
SELECT * FROM productos;
SELECT * FROM ventas;
```
Los registros deben aparecer normalizados y sin duplicados.

**ğŸ“Š 7. AnÃ¡lisis de datos en Jupyter**

Desde la carpeta notebooks/:

```bash
jupyter notebook
```
Abrir:
```bash
ventas_analysis.ipynb
```
El notebook:

* Se conecta a PostgreSQL
* Realiza JOINs
* Ejecuta anÃ¡lisis exploratorio
* No modifica datos

âš ï¸ Orden correcto de ejecuciÃ³n

* PostgreSQL activo
* API de Python (limpieza)
* API de Node.js (ingestiÃ³n)
* Postman / Jupyter

Si la API de Python y Node no estÃ¡ levantada, la ingestiÃ³n fallarÃ¡.

**ğŸ§  Notas importantes**
* Cada servicio corre de forma independiente
* La comunicaciÃ³n entre Node y Python simula un entorno de microservicios
* Todo el flujo es sÃ­ncrono y local

